{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Horovod_Distributed_Gradient_Descent_Exercise",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPUpyQ0UrnTi"
      },
      "source": [
        "# Distributed Gradient Descent with Horovod \n",
        "### Explained using Multivariate Linear Regression in PyTorch and Python\n",
        "\n",
        "by <a href=\"mailto:carl.osipov@gmail.com\"><b>Carl Osipov</b></a>, based on the materials from his <a href=\"https://bit.ly/cnml-ebook\"><b>\"Cloud Native Machine Learning\"</b></a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CGr_ILLjs-5"
      },
      "source": [
        "In this notebook, you can learn about:\n",
        "\n",
        "* gradient descent with gradient accumulation \n",
        "* deep learning with training datasets that do not fit in node memory\n",
        "* gradient accumulation in parameter server-based vs ring-based (for example Horovod) gradient descent\n",
        "* reduce-scatter and reduce-all phases in Horovod\n",
        "* Horovod as an implementation for distributed data parallel training in deep learning\n",
        "\n",
        "Why should you care? Are you:\n",
        "\n",
        "* informing deep learning platform buy vs. build decision for your project, team, or organization\n",
        "* scaling up your deep learning models to out-of-memory training datasets\n",
        "* understanding or troubleshooting distributed deep learning training\n",
        "* researching the direction of high performance distributed deep learning \n",
        "* working on federated deep learning or bandwidth-constrained (IoT) training at cloud's edge\n",
        "\n",
        "This notebook builds on the gradient descent, automatic differentiation, tensors, and other concepts introduced in:\n",
        "\n",
        "* <a href=\"https://colab.research.google.com/github/osipov/manning/blob/master/autodiff/Solution_Autodiff_Algorithm.ipynb\">Automatic Differentiation Explained</a>\n",
        "* <a href=\"https://web.archive.org/web/20200925082637/https://www.cs.fsu.edu/~xyuan/paper/09jpdc.pdf\">Bandwidth Optimal All-reduce Algorithms</a>\n",
        "* <a href=\"https://web.archive.org/web/20200413112232/https://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/\">Bringing HPC Techniques to Deep Learning</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPQatPwX4_xm"
      },
      "source": [
        "### Import a standard set of libraries and initialize the pseudo-random number generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1xihKxxiVSc"
      },
      "source": [
        "import torch as pt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "SEED = 42\n",
        "pt.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "pt.__version__, np.__version__, pd.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEr6DvUC_kF9"
      },
      "source": [
        "### Create a training dataset of features for multivariate linear regression\n",
        "\n",
        "* `TRAINING_DATASET_SIZE` is pre-set to a default of 1,000\n",
        "* Assume that `FEATURES` is set to 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UBNxn0IiCvr"
      },
      "source": [
        "TRAINING_DATASET_SIZE = 1000\n",
        "FEATURES = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEZQuzgVOYfq"
      },
      "source": [
        "Create a NumPy nd-array named `X_numpy` with a shape of `(TRAINING_DATASET_SIZE, FEATURES)`:\n",
        "\n",
        "* the array should contain feature values such that the values in each column are from a normal (bell-shaped) distribution. \n",
        "* the means of the feature values should be `0.0` for the first column, `1.0` for the second column, and so on.\n",
        "* the feature values across columns should be uncorrelated.\n",
        "\n",
        "**Hint:** You can use `np.random.multivariate_normal` to create `X_numpy`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ClluXKlOcg0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEeGW0kJ8ucF"
      },
      "source": [
        "### Check that a column's feature values are normal\n",
        "\n",
        "**Hint:** You can use the `matplotlib.pyplot.hist` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0O1CEIavmqb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L53xb1yJ9FzC"
      },
      "source": [
        "### Confirm that the feature values are uncorrelated across columns\n",
        "\n",
        "**Hint:** You can use the `pandas.DataFrame.corr` method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5x1JpRGwE3q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3IHusG99RYb"
      },
      "source": [
        "### Create a PyTorch tensor `X_train` from `X_numpy`\n",
        "* You can use PyTorch's `from_numpy` function to re-use the in-memory data allocated by a NumPy nd-array.\n",
        "* Ensure that the resulting PyTorch tensor has a shape of `(TRAINING_DATASET_SIZE, FEATURES)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1pvHea3qCoP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ndurdli7PoaX"
      },
      "source": [
        "Note that the `COEFFICIENTS` tensor is pre-defined as `[5, 3, 2, 1]` just to make the linear regression easier to visualize in the upcoming cells.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7j8AjB6OPqpc"
      },
      "source": [
        "COEFFICIENTS = pt.tensor([5, 3, 2, 1])\n",
        "COEFFICIENTS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mp6DAkRW9aye"
      },
      "source": [
        "### Create a `y_train` tensor using `COEFFICIENTS` and `X_train`\n",
        "\n",
        "\n",
        "Gradient descent will attempt to recover the `COEFFICIENTS` using just the `y_train` and `X_train` values.\n",
        "\n",
        "* Ensure that `y_train` has a shape of `(TRAINING_DATASET_SIZE)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hp31KUAiqGn_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4G-EKzK-ECl"
      },
      "source": [
        "### Visualize the 5 dimensions of the linear regression problem\n",
        "\n",
        "Use the scatter plot to visualize the 4 dimensions of the `X_train` as 3 spatial and 1 size dimensions. The 1 dimension of `y_train` is visualized as the color on the scatter plot.\n",
        "\n",
        "* the `SCALE` setting is configured to 1,000 for a better looking visualization and does not change the original multivariate linear regression problem.\n",
        "\n",
        "* the `STRIDE` setting is configured to 25 by default. You can choose more data points out of `TRAINING_DATASET_SIZE` to visualize by setting `STRIDE` to be closer to 1, or fewer data points to visualize by using a `STRIDE` value closer to `TRAINING_DATASET_SIZE`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4ggqaBkigOe"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "figure = plt.figure(figsize = (16, 9))\n",
        "axis = Axes3D(figure, rect = [0, 0, .95, 1], elev=48, azim=134)\n",
        "\n",
        "SCALE = 1_000\n",
        "y_view, X_view = SCALE * y_train, SCALE * X_train\n",
        "\n",
        "STRIDE = 25\n",
        "pc = axis.scatter(X_view[::STRIDE, 0], X_view[::STRIDE, 1], X_view[::STRIDE, 2], s = X_view[::STRIDE, 3], c = y_view[::STRIDE], cmap='autumn', alpha=0.4)\n",
        "figure.colorbar(pc, fraction=0.01)\n",
        "axis.set_xlabel('x0'), axis.set_ylabel('x1'), axis.set_zlabel('x2');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z004ZY-YLNKo"
      },
      "source": [
        "### Create random values for the initial model parameters\n",
        "* create a `w_numpy` nd-array shaped `(FEATURES, 1)` with the model parameter values\n",
        "* you can use normally distributed `randn` values or try a more complex initialization scheme"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec3aalSyK9pM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5Cc5-sb_1ls"
      },
      "source": [
        "### Instantiate a differentiable tensor `w` as the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwWVHhtlLARM"
      },
      "source": [
        "* the `w` tensor should use `w_numpy` and use `pt.float64` as the `dtype`\n",
        "* recall that `requires_grad` must be `True` for a PyTorch tensor to be differentiable\n",
        "* you can use `torch.randn` to initialize the values or try a more complex scheme like `torch.nn.init.kaiming_uniform_`\n",
        "* implement the `forward` and `mse` functions for the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMyMWeb_3HXC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h23pV490Bhk0"
      },
      "source": [
        "### Solve for the coefficients using ordinary <a href=\"https://en.wikipedia.org/wiki/Gradient_descent\">gradient descent</a>\n",
        "\n",
        "For consistency, with the upcoming parts of the notebook use \n",
        "\n",
        "* `EPOCHS = 400`\n",
        "* `LEARNING_RATE = 0.01`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPqi6M0Tuoy_"
      },
      "source": [
        "EPOCHS = 400\n",
        "LEARNING_RATE = 0.01"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJXoLYzZurii"
      },
      "source": [
        "Recover the coefficients from the data using ordinary gradient descent\n",
        "* the entire training dataset is used to compute the gradients per iteration of gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk8ufpOPBqAb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IME5dGwixiM7"
      },
      "source": [
        "**What if the training dataset doesn't fit in-memory of the node?**\n",
        "\n",
        "### Gradient Descent using Gradient Accumulation\n",
        "\n",
        "* Single node, in-memory implementation\n",
        "* For example, an in-memory **shard** (a part of the training dataset) can consist of 250 training examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkyRszjdWiDz"
      },
      "source": [
        "<img src=\"https://i.imgur.com/cBgNUL2.png\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhjjFMgpb47R"
      },
      "source": [
        "* Use `IN_MEMORY_SHARD_SIZE` of `250` to demonstrate node's memory limitation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fLXEJCZ1qoK"
      },
      "source": [
        "IN_MEMORY_SHARD_SIZE = 250\n",
        "\n",
        "w = pt.tensor(w_numpy, requires_grad=True, dtype=pt.float64)\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19mCTZScCHpH"
      },
      "source": [
        "### Single node gradient descent with gradient accumulation\n",
        "\n",
        "* Use `TensorDataset` and `DataLoader` from the `torch.utils.data` package\n",
        "* Use `IN_MEMORY_SHARD_SIZE / TRAINING_DATASET_SIZE` to adjust the loss function for the shard size \n",
        "* Demonstrate gradient accumulation vs mini-batch gradient descent explaining shards vs. training batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UaMBPobFqE_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m86uRqdDGu_M"
      },
      "source": [
        "**Can gradient accumulation help distribute gradient descent?**\n",
        "\n",
        "# Parameter server-based distributed (multi-node) gradient descent\n",
        "\n",
        "<img src=\"https://i.imgur.com/Y1wDyRs.png\"/>\n",
        "\n",
        "* straightforward distributed architecture, popularized by TensorFlow 1.x and Google Cloud AI Platform\n",
        "* **Distributed Data Parallel** machine learning training approach where the same model parameters are used across all nodes during training\n",
        "* model **MUST** fit in-memory of the worker and parameter nodes\n",
        "* parallel processing (shard to gradients) improves gradient step latency vs sequential gradient accumulation\n",
        "* bandwidth inefficient (bottlenecked) due to many-to-one (worker nodes to parameter nodes) gradient transfer and one-to-many (parameter nodes to worker nodes) model parameter transfer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4hGG9EfdWjm"
      },
      "source": [
        "**Can we do better than the parameter server-based approach?**\n",
        "\n",
        "### Intuition behind ring-based distributed gradient descent\n",
        "\n",
        "* Introduce `NODES` as `TRAINING_DATASET_SIZE // IN_MEMORY_SHARD_SIZE` for distributed gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZRE1DiesKLF"
      },
      "source": [
        "NODES = TRAINING_DATASET_SIZE // IN_MEMORY_SHARD_SIZE\n",
        "\n",
        "GRADIENTS = [5, 3, 2, 1]\n",
        "GRADIENTS\n",
        "node_to_gradients = dict(zip(range(NODES), GRADIENTS))\n",
        "node_to_gradients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM_TKjb-rdvq"
      },
      "source": [
        "#### All-Gather Operation Example (not Horovod)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jgnNcdxeUlh"
      },
      "source": [
        "<img src=\"https://i.imgur.com/vHfxzQt.png\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9y1ea7OxnrtO"
      },
      "source": [
        "### Demonstrate 3 iterations with 4 nodes\n",
        "\n",
        "* Assuming iteration 0 has node 1 start the communication, after 3 iterations the gradients should be accumulated (reduced) on node 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMkxhvtuH-h2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpmtgGg00cgD"
      },
      "source": [
        "# Horovod: A ring-based distributed gradient descent\n",
        "\n",
        "* **Distributed Data Parallel** approach using (1) reduce-scatter and (2) all-gather phases\n",
        "* more bandwidth efficient than parameter server based approaches and plain all-gather\n",
        "* not to be confused with <a href=\"https://github.com/horovod/horovod\">Hovorod</a> the machine learning framework\n",
        "* also not to be confused with <a href=\"https://i.imgur.com/TVDynOb.jpeg\">Horovod the Slavic folk dance</a> which inspired the name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbY0hoktlahD"
      },
      "source": [
        "### Partitioning gradients into \"segments\"\n",
        "\n",
        "* having a **segment** of the gradients enables Horovod to decrease the amount of bandwidth needed to complete the reduce-all operation, in other words, to deliver the sum of the gradients for each shard to every node in the ring.\n",
        "\n",
        "* by default there are as many segments  as nodes\n",
        "\n",
        "<img src=\"https://i.imgur.com/dyj2LMU.png\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3jrRd9aiLjI"
      },
      "source": [
        "# Gradient descent with Horovod"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc9ZRiTs5_0P"
      },
      "source": [
        "* create a list of tensors `W` with `NODES` tensors based on `w_numpy` values\n",
        "* recall that each of the tensors in the list `W` must have `requires_grad` set to `True`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPfW4LFtzOQE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVI2iEHl6jX4"
      },
      "source": [
        "## Perform a single forward pass of gradient descent for every node\n",
        "* you use `zip` on `range(NODES)` and `train_dl`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfsdpnUqnrLM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zl9H15-Q7qFy"
      },
      "source": [
        "### Output the gradient segments on each node"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcONCTEL77-K"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT0Gam4r7efY"
      },
      "source": [
        "### Output the target sum of the gradients as the target for Horovod"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hWsQ2bz7esw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfzuCGME7yCc"
      },
      "source": [
        "## Horovod Phase 1: Reduce Scatter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMF56tSF8HDU"
      },
      "source": [
        "%%html\n",
        "<video  width=\"960\" height=\"720\" autoplay loop muted playsinline controls>\n",
        "    <source src=\"https://i.imgur.com/IV6jBwL.mp4\" type=\"video/mp4\">\n",
        "</video>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wP8UYUb6o9q1"
      },
      "source": [
        "#horovod phase 1: reduce scatter\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rv774p7k8RA0"
      },
      "source": [
        "### Output the gradient segments on each node"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWOn6G728RHq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TdQhmj5-qh7"
      },
      "source": [
        "## Output just the reduced (accumulated) segment on each node"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_A4koxY8nbk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5yyRpV68hTf"
      },
      "source": [
        "## Horovod Phase 2: All Gather"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o27PJAjR8nKE"
      },
      "source": [
        "%%html\n",
        "<video  width=\"960\" height=\"720\" autoplay loop muted playsinline controls>\n",
        "    <source src=\"https://i.imgur.com/VU87GmZ.mp4\" type=\"video/mp4\">\n",
        "</video>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drHeGxbsy-T2"
      },
      "source": [
        "#horovod phase 2: all gather\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXXh-Ip0j1pp"
      },
      "source": [
        "# Bringing it all together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zujT0yss-kj0"
      },
      "source": [
        "W = [pt.tensor(w_numpy, requires_grad=True) for _ in range(NODES)]\n",
        "\n",
        "EPOCHS = 400\n",
        "for epoch in range(EPOCHS): \n",
        "  \n",
        "  #compute per batch gradients on each node\n",
        "  for node, (y_shard, X_shard) in zip(range(NODES), train_dl):\n",
        "    y_est = forward(W[node], X_shard)\n",
        "    loss = (IN_MEMORY_SHARD_SIZE / TRAINING_DATASET_SIZE) * mse(y_shard, y_est)\n",
        "    loss.backward()\n",
        "\n",
        "  #horovod phase 1: reduce-scatter\n",
        "  for iter in range(NODES - 1):\n",
        "    for node in range(NODES):\n",
        "      seg = (node - iter - 1) % NODES\n",
        "      grad = W[node].grad[seg]\n",
        "\n",
        "      next_node = (node + 1) % NODES\n",
        "      W[next_node].grad[seg] += grad\n",
        "\n",
        "  #horovod phase 2: all-gather\n",
        "  for iter in range(NODES - 1):\n",
        "    for node in range(NODES):\n",
        "      seg = (node - iter) % NODES\n",
        "      grad = W[node].grad[seg]\n",
        "\n",
        "      next_node = (node + 1) % NODES\n",
        "      W[next_node].grad[seg] = grad\n",
        "\n",
        "  #perform a step of gradient descent\n",
        "  for node in range(NODES):\n",
        "    W[node].data -= LEARNING_RATE * W[node].grad\n",
        "    W[node].grad = None\n",
        "\n",
        "for node in range(NODES):\n",
        "  print(W[node].data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gd1y1q_HVuin"
      },
      "source": [
        "Copyright 2020 CounterFactual.AI LLC. All Rights Reserved.\n",
        "\n",
        "Licensed under the GNU General Public License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. \n",
        "\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "https://github.com/osipov/smlbook/blob/master/LICENSE\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
      ]
    }
  ]
}